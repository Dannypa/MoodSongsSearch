{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying manual tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras, keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "import bert.tokenization as tokenization\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "config_json = {\n",
    "    \"bert\": 'C:\\\\Users\\dannu\\\\bert-base',\n",
    "}\n",
    "config_json['bert_config'] = os.path.join(config_json['bert'], 'bert_config.json')\n",
    "config_json['checkpoint'] = os.path.join(config_json['bert'], 'bert_model.ckpt')\n",
    "config_json['vocab'] = os.path.join(config_json['bert'], 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file=config_json['vocab'], do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(\"\"\"\n",
    "The sun is nearly gone\n",
    "The lights are turning on\n",
    "A silver shine that stretches to the sea\n",
    "We've stumbled on a view\n",
    "That's tailor-made for two\n",
    "What a shame those two are you and me\n",
    "Some other girl and guy\n",
    "Would love this swirling sky\n",
    "But there's only you and I\n",
    "And we've got no shot\n",
    "This could never be\n",
    "You're not the type for me (really?)\n",
    "And there's not a spark in sight\n",
    "What a waste of a lovely night\n",
    "You say there's nothing here?\n",
    "Well, let's make something clear\n",
    "I think I'll be the one to make that call (but you'll call?)\n",
    "And though you looked so cute\n",
    "In your polyester suit (it's wool)\n",
    "You're right, I'd never fall for you at all\n",
    "And maybe this appeals\n",
    "To someone not in heels\n",
    "Or to any girl who feels\n",
    "There's some chance for romance\n",
    "But, I'm frankly feeling nothing\n",
    "Is that so?\n",
    "Or it could be less than nothing\n",
    "Good to know, so you agree?\n",
    "That's right\n",
    "What a waste of a lovely night\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_input = tokenizer.convert_tokens_to_ids(tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids([\"king\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids([\"queen\"])[0] - tokenizer.convert_tokens_to_ids(\"woman\") [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually, i'd rather try old thing, but a better version of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "transformer = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "DB_DIR = \"db_lyrics\"\n",
    "remake = True\n",
    "if remake:\n",
    "    data = []\n",
    "    for file in os.listdir(DB_DIR):\n",
    "        with open(os.path.join(DB_DIR, file), 'r') as f:\n",
    "            cur = json.load(f)\n",
    "            for el in cur['data']:\n",
    "                data.append((el['lyrics'], el['meta']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "DB_DIR = \"db_lyrics\"\n",
    "remake = True\n",
    "if remake:\n",
    "    song_data = []\n",
    "    for file in os.listdir(DB_DIR):\n",
    "        with open(os.path.join(DB_DIR, file), 'r') as f:\n",
    "            cur = json.load(f)\n",
    "            for el in cur['data']:\n",
    "                song_data.append((el['song'], el['artist']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11559"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(song_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5674/5674 [2:10:49<00:00,  1.38s/it]  \n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from threading import Thread\n",
    "\n",
    "remake = False\n",
    "\n",
    "if remake:\n",
    "    def fill_encoded(index):\n",
    "        encoded_data[index] = (transformer.encode(data[index][0]), data[index][1])\n",
    "\n",
    "    step = 2\n",
    "    for i in tqdm.trange(0, len(data), step):\n",
    "        threads = [Thread(target=fill_encoded, args=(i + j,)) for j in range(min(step, len(data) - i))]\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        # break\n",
    "        # encoded_data[i] = (transformer.encode(data[i][0]), data[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['danceability', 'energy', 'key', 'loudness', 'mode', 'valence', 'tempo'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data[0][1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"all-mpnet-base-v2-data-new\"\n",
    "remake = False\n",
    "keys = ['danceability', 'energy', 'key', 'loudness', 'mode', 'valence', 'tempo']\n",
    "if remake:\n",
    "    cur = []\n",
    "    index = 0\n",
    "    for el in encoded_data:\n",
    "        cur.append({\"lyrics\": [x.item() for x in el[0]], \"meta\": [el[1][k] for k in keys]})\n",
    "        # print(cur)\n",
    "        # break\n",
    "        if len(cur) >= 1000:\n",
    "            filename = os.path.join(DATA_DIR, f\"data-{index}.json\")\n",
    "            index += 1\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(cur, f)\n",
    "            cur = []\n",
    "    filename = os.path.join(DATA_DIR, f\"data-{index}.json\")\n",
    "    index += 1\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(cur, f)\n",
    "    cur = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "DATA_DIR = \"all-mpnet-base-v2-data\"\n",
    "lyrics_max_val = 3.0483362674713135\n",
    "meta_max_val = [torch.tensor(0.9880), torch.tensor(1.), torch.tensor(11.), torch.tensor(60.), torch.tensor(1.), torch.tensor(0.9970), torch.tensor(220.1690)]\n",
    "    \n",
    "vecs = []\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    if file.endswith('.json'):\n",
    "        filename = os.path.join(DATA_DIR, file)\n",
    "        with open(filename, 'r') as f:\n",
    "            cur = json.load(f)\n",
    "            for el in cur:\n",
    "                meta = el['meta']\n",
    "                for j in range(len(meta)):\n",
    "                    meta[j] /= meta_max_val[j]\n",
    "                lyrics = el['lyrics']\n",
    "                for j in range(len(lyrics)):\n",
    "                    lyrics[j] /= lyrics_max_val\n",
    "                vecs.append((np.array(lyrics, dtype=np.float32), np.array(meta, dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rn scaling uses hardcoded values because the calculations of maximum values took to much time; it is the version that can be used to recalc values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data():\n",
    "    global dataset\n",
    "    n = len(dataset)\n",
    "    # may be scaling is not needed.\n",
    "    first_time = False\n",
    "    if first_time:\n",
    "        lyrics_max_val = 0\n",
    "        meta_max_val = [0 for i in range(7)]\n",
    "\n",
    "        for data in tqdm(dataset):\n",
    "        #     print(data[0])\n",
    "        #     print(data[1])\n",
    "    #         break\n",
    "            lyrics_max_val = max(lyrics_max_val, max(data[0]).item())\n",
    "            for i in range(7):\n",
    "                meta_max_val[i] = max(meta_max_val[i], abs(data[1][i]))\n",
    "    lyrics_max_val = 3.0483362674713135\n",
    "    meta_max_val = [torch.tensor(0.9880), torch.tensor(1.), torch.tensor(11.), torch.tensor(60.), torch.tensor(1.), torch.tensor(0.9970), torch.tensor(220.1690)]\n",
    "    print(lyrics_max_val)\n",
    "    print(meta_max_val)\n",
    "\n",
    "\n",
    "    first_time = True\n",
    "    if first_time:\n",
    "        # c = input(\"continue?(y/n)>>\")\n",
    "        c= 'y'\n",
    "        if c.lower() == 'y':\n",
    "            for data in tqdm(dataset):\n",
    "                if lyrics_max_val > 1:\n",
    "                    data[0] /= lyrics_max_val\n",
    "                for i in range(7):\n",
    "                    if meta_max_val[i] > 1:\n",
    "                        data[1][i] /= meta_max_val[i]\n",
    "    print(dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating data into np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vecs = [(np.array([x.item() for x in el[0]], dtype=np.float32), np.array([el[1][k] for k in keys], dtype=np.float32)) for el in encoded_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting data into dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n = len(vecs)\n",
    "train_set, test_set = [], []\n",
    "train_loader, test_loader = [], []\n",
    "def split_data():\n",
    "    global train_set, test_loader, test_set, train_loader\n",
    "    train_size = int(n * .9)\n",
    "    train_set = vecs[:train_size]\n",
    "    test_set = vecs[train_size:]\n",
    "    # len(train_set), len(test_set)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=config_json['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=config_json['batch_size'], shuffle=False)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Net\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import src.config as config\n",
    "import time\n",
    "def train_loop(model, criterion, optimizer, train_loader, n):\n",
    "    loss_avg = AverageMeter()\n",
    "    acc_stat = AverageMeter()\n",
    "    start_time = time.time()\n",
    "    for embeddings, targets in train_loader:\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        embeddings = embeddings.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        output = model(embeddings)\n",
    "        loss = criterion(output, targets)\n",
    "        loss_avg.update(loss.item(), 1)\n",
    "\n",
    "        output2 = output.softmax(dim=1)\n",
    "        output2 = output2.cpu().detach().numpy()\n",
    "\n",
    "        acc = np.linalg.norm(targets - output2)\n",
    "\n",
    "        acc_stat.update(acc, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr = param_group['lr']\n",
    "    print(f\"{n} epoch, Loss: {loss_avg.avg:.5f}, acc: {acc_stat.avg:.4f}, LR: {lr:.7f}, Time for cycle: {(time.time() - start_time):.2f} sec\")\n",
    "    return loss_avg.avg\n",
    "\n",
    "import tqdm\n",
    "\n",
    "def model_load(preTrained=False, checkpoint_path=\"\"):\n",
    "    model = Net()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    if preTrained:\n",
    "        \"\"\"Loading model\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE )\n",
    "        model.load_state_dict(     checkpoint['model'])\n",
    "        model=model.to(DEVICE)\n",
    "        # optimizer.load_state_dict( checkpoint['optimizer']) # i want to change lr\n",
    "        epoch =                    checkpoint['epoch']\n",
    "        score =                    checkpoint['score']\n",
    "        return model, optimizer, criterion, epoch, score\n",
    "    else:\n",
    "        return model.to(DEVICE), optimizer, criterion, 0, 0\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, epoch, score, path):\n",
    "    checkpoint={\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'score': score\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "def Train_log(score, path):\n",
    "    arr=np.array([score])\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'rb') as f:\n",
    "            arr = np.load(f)\n",
    "        arr = np.append(arr, score)\n",
    "    with open(path, 'wb') as f:\n",
    "        np.save(f, arr)\n",
    "\n",
    "\n",
    "def train(second_time):\n",
    "    model, optimizer, criterion, epoch, score = model_load(config, second_time, config.CHECKPOINT_PATH)\n",
    "    print(\"Model loaded\")\n",
    "    criterion = criterion.to(DEVICE)\n",
    "    model=model.to(DEVICE)\n",
    "    for e in range(100):\n",
    "        score = train_loop(model, criterion, optimizer, train_loader, e)\n",
    "        save_model(model, optimizer, e, score, config.CHECKPOINT_PATH)\n",
    "        Train_log(score, config.TRAIN_LOG)\n",
    "        print(\"Data saved \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "0 epoch, Loss: 0.02578, acc: 28.1108, LR: 0.0010000, Time for cycle: 0.62 sec\n",
      "Data saved \n",
      "\n",
      "1 epoch, Loss: 0.01733, acc: 28.0538, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "2 epoch, Loss: 0.01632, acc: 28.0184, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "3 epoch, Loss: 0.01616, acc: 28.0095, LR: 0.0010000, Time for cycle: 0.58 sec\n",
      "Data saved \n",
      "\n",
      "4 epoch, Loss: 0.01610, acc: 28.0133, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "5 epoch, Loss: 0.01615, acc: 28.0088, LR: 0.0010000, Time for cycle: 0.53 sec\n",
      "Data saved \n",
      "\n",
      "6 epoch, Loss: 0.01618, acc: 28.0156, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "7 epoch, Loss: 0.01615, acc: 28.0091, LR: 0.0010000, Time for cycle: 0.58 sec\n",
      "Data saved \n",
      "\n",
      "8 epoch, Loss: 0.01624, acc: 28.0164, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "9 epoch, Loss: 0.01618, acc: 28.0142, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "10 epoch, Loss: 0.01611, acc: 28.0243, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "11 epoch, Loss: 0.01605, acc: 28.0085, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "12 epoch, Loss: 0.01612, acc: 28.0195, LR: 0.0010000, Time for cycle: 0.53 sec\n",
      "Data saved \n",
      "\n",
      "13 epoch, Loss: 0.01606, acc: 28.0199, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "14 epoch, Loss: 0.01612, acc: 28.0159, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "15 epoch, Loss: 0.01611, acc: 28.0214, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "16 epoch, Loss: 0.01616, acc: 28.0184, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "17 epoch, Loss: 0.01630, acc: 28.0244, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "18 epoch, Loss: 0.01624, acc: 28.0147, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "19 epoch, Loss: 0.01616, acc: 28.0241, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "20 epoch, Loss: 0.01610, acc: 28.0148, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "21 epoch, Loss: 0.01616, acc: 28.0192, LR: 0.0010000, Time for cycle: 0.53 sec\n",
      "Data saved \n",
      "\n",
      "22 epoch, Loss: 0.01615, acc: 28.0031, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "23 epoch, Loss: 0.01616, acc: 28.0136, LR: 0.0010000, Time for cycle: 0.58 sec\n",
      "Data saved \n",
      "\n",
      "24 epoch, Loss: 0.01623, acc: 28.0174, LR: 0.0010000, Time for cycle: 0.61 sec\n",
      "Data saved \n",
      "\n",
      "25 epoch, Loss: 0.01613, acc: 28.0189, LR: 0.0010000, Time for cycle: 0.57 sec\n",
      "Data saved \n",
      "\n",
      "26 epoch, Loss: 0.01599, acc: 28.0047, LR: 0.0010000, Time for cycle: 0.53 sec\n",
      "Data saved \n",
      "\n",
      "27 epoch, Loss: 0.01594, acc: 28.0199, LR: 0.0010000, Time for cycle: 0.58 sec\n",
      "Data saved \n",
      "\n",
      "28 epoch, Loss: 0.01600, acc: 28.0053, LR: 0.0010000, Time for cycle: 0.57 sec\n",
      "Data saved \n",
      "\n",
      "29 epoch, Loss: 0.01596, acc: 28.0092, LR: 0.0010000, Time for cycle: 0.53 sec\n",
      "Data saved \n",
      "\n",
      "30 epoch, Loss: 0.01606, acc: 28.0175, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "31 epoch, Loss: 0.01608, acc: 28.0144, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "32 epoch, Loss: 0.01604, acc: 28.0127, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "33 epoch, Loss: 0.01598, acc: 28.0151, LR: 0.0010000, Time for cycle: 0.57 sec\n",
      "Data saved \n",
      "\n",
      "34 epoch, Loss: 0.01599, acc: 28.0129, LR: 0.0010000, Time for cycle: 0.58 sec\n",
      "Data saved \n",
      "\n",
      "35 epoch, Loss: 0.01585, acc: 28.0138, LR: 0.0010000, Time for cycle: 0.60 sec\n",
      "Data saved \n",
      "\n",
      "36 epoch, Loss: 0.01594, acc: 28.0158, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "37 epoch, Loss: 0.01608, acc: 28.0078, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "38 epoch, Loss: 0.01595, acc: 28.0117, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "39 epoch, Loss: 0.01595, acc: 28.0217, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "40 epoch, Loss: 0.01596, acc: 28.0173, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "41 epoch, Loss: 0.01586, acc: 28.0093, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "42 epoch, Loss: 0.01581, acc: 28.0150, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "43 epoch, Loss: 0.01582, acc: 28.0136, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "44 epoch, Loss: 0.01580, acc: 28.0083, LR: 0.0010000, Time for cycle: 0.53 sec\n",
      "Data saved \n",
      "\n",
      "45 epoch, Loss: 0.01580, acc: 28.0128, LR: 0.0010000, Time for cycle: 0.58 sec\n",
      "Data saved \n",
      "\n",
      "46 epoch, Loss: 0.01597, acc: 28.0204, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "47 epoch, Loss: 0.01596, acc: 28.0206, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "48 epoch, Loss: 0.01594, acc: 28.0048, LR: 0.0010000, Time for cycle: 0.57 sec\n",
      "Data saved \n",
      "\n",
      "49 epoch, Loss: 0.01618, acc: 28.0113, LR: 0.0010000, Time for cycle: 0.61 sec\n",
      "Data saved \n",
      "\n",
      "50 epoch, Loss: 0.01588, acc: 28.0072, LR: 0.0010000, Time for cycle: 0.58 sec\n",
      "Data saved \n",
      "\n",
      "51 epoch, Loss: 0.01584, acc: 28.0022, LR: 0.0010000, Time for cycle: 0.60 sec\n",
      "Data saved \n",
      "\n",
      "52 epoch, Loss: 0.01583, acc: 27.9973, LR: 0.0010000, Time for cycle: 0.58 sec\n",
      "Data saved \n",
      "\n",
      "53 epoch, Loss: 0.01578, acc: 27.9995, LR: 0.0010000, Time for cycle: 0.64 sec\n",
      "Data saved \n",
      "\n",
      "54 epoch, Loss: 0.01565, acc: 28.0061, LR: 0.0010000, Time for cycle: 0.61 sec\n",
      "Data saved \n",
      "\n",
      "55 epoch, Loss: 0.01595, acc: 28.0077, LR: 0.0010000, Time for cycle: 0.59 sec\n",
      "Data saved \n",
      "\n",
      "56 epoch, Loss: 0.01581, acc: 28.0099, LR: 0.0010000, Time for cycle: 0.59 sec\n",
      "Data saved \n",
      "\n",
      "57 epoch, Loss: 0.01570, acc: 28.0113, LR: 0.0010000, Time for cycle: 0.61 sec\n",
      "Data saved \n",
      "\n",
      "58 epoch, Loss: 0.01584, acc: 28.0186, LR: 0.0010000, Time for cycle: 0.60 sec\n",
      "Data saved \n",
      "\n",
      "59 epoch, Loss: 0.01564, acc: 28.0023, LR: 0.0010000, Time for cycle: 0.57 sec\n",
      "Data saved \n",
      "\n",
      "60 epoch, Loss: 0.01572, acc: 28.0014, LR: 0.0010000, Time for cycle: 0.60 sec\n",
      "Data saved \n",
      "\n",
      "61 epoch, Loss: 0.01551, acc: 28.0151, LR: 0.0010000, Time for cycle: 0.64 sec\n",
      "Data saved \n",
      "\n",
      "62 epoch, Loss: 0.01559, acc: 27.9975, LR: 0.0010000, Time for cycle: 0.57 sec\n",
      "Data saved \n",
      "\n",
      "63 epoch, Loss: 0.01554, acc: 28.0083, LR: 0.0010000, Time for cycle: 0.57 sec\n",
      "Data saved \n",
      "\n",
      "64 epoch, Loss: 0.01574, acc: 28.0103, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "65 epoch, Loss: 0.01561, acc: 28.0047, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "66 epoch, Loss: 0.01560, acc: 27.9986, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "67 epoch, Loss: 0.01547, acc: 27.9910, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "68 epoch, Loss: 0.01547, acc: 28.0119, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "69 epoch, Loss: 0.01552, acc: 28.0049, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "70 epoch, Loss: 0.01568, acc: 27.9983, LR: 0.0010000, Time for cycle: 0.57 sec\n",
      "Data saved \n",
      "\n",
      "71 epoch, Loss: 0.01538, acc: 28.0152, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "72 epoch, Loss: 0.01537, acc: 28.0072, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "73 epoch, Loss: 0.01540, acc: 28.0110, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "74 epoch, Loss: 0.01539, acc: 28.0062, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "75 epoch, Loss: 0.01570, acc: 28.0160, LR: 0.0010000, Time for cycle: 0.59 sec\n",
      "Data saved \n",
      "\n",
      "76 epoch, Loss: 0.01568, acc: 28.0106, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "77 epoch, Loss: 0.01539, acc: 28.0050, LR: 0.0010000, Time for cycle: 0.70 sec\n",
      "Data saved \n",
      "\n",
      "78 epoch, Loss: 0.01563, acc: 28.0093, LR: 0.0010000, Time for cycle: 0.57 sec\n",
      "Data saved \n",
      "\n",
      "79 epoch, Loss: 0.01625, acc: 28.0203, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "80 epoch, Loss: 0.01564, acc: 28.0060, LR: 0.0010000, Time for cycle: 0.60 sec\n",
      "Data saved \n",
      "\n",
      "81 epoch, Loss: 0.01529, acc: 28.0051, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "82 epoch, Loss: 0.01541, acc: 28.0101, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "83 epoch, Loss: 0.01537, acc: 28.0071, LR: 0.0010000, Time for cycle: 0.62 sec\n",
      "Data saved \n",
      "\n",
      "84 epoch, Loss: 0.01606, acc: 28.0097, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "85 epoch, Loss: 0.01572, acc: 28.0051, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "86 epoch, Loss: 0.01529, acc: 28.0079, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "87 epoch, Loss: 0.01519, acc: 27.9931, LR: 0.0010000, Time for cycle: 0.53 sec\n",
      "Data saved \n",
      "\n",
      "88 epoch, Loss: 0.01510, acc: 27.9932, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "89 epoch, Loss: 0.01508, acc: 28.0052, LR: 0.0010000, Time for cycle: 0.52 sec\n",
      "Data saved \n",
      "\n",
      "90 epoch, Loss: 0.01499, acc: 27.9958, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "91 epoch, Loss: 0.01499, acc: 28.0018, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "92 epoch, Loss: 0.01497, acc: 28.0057, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "93 epoch, Loss: 0.01497, acc: 28.0032, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "94 epoch, Loss: 0.01490, acc: 27.9981, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "95 epoch, Loss: 0.01498, acc: 28.0109, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "96 epoch, Loss: 0.01493, acc: 27.9989, LR: 0.0010000, Time for cycle: 0.54 sec\n",
      "Data saved \n",
      "\n",
      "97 epoch, Loss: 0.01495, acc: 28.0082, LR: 0.0010000, Time for cycle: 0.55 sec\n",
      "Data saved \n",
      "\n",
      "98 epoch, Loss: 0.01498, acc: 27.9938, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n",
      "99 epoch, Loss: 0.01490, acc: 27.9986, LR: 0.0010000, Time for cycle: 0.56 sec\n",
      "Data saved \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    checkpoint = torch.load(config.CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    m = Net()\n",
    "    m.load_state_dict(checkpoint['model'])\n",
    "    torch.save(m.state_dict(), os.path.join(config.SAVE_DIR, f\"{config.MODEL_NAME}.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just some testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance:  0.683\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    def euclid_distance(a, b):\n",
    "        assert len(a) == len(b)\n",
    "        return np.sqrt(np.sum([\n",
    "            (xa - xb) ** 2 for xa, xb, in zip(a, b)\n",
    "        ]))\n",
    "\n",
    "\n",
    "    _sum = 0\n",
    "    num = 0\n",
    "    mx = 0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for data in test_loader:\n",
    "            X, y = data\n",
    "            # print(y)\n",
    "            output = m(X.view(-1,768))\n",
    "            #print(output)\n",
    "            for idx, res in enumerate(output):\n",
    "                # print(y[idx])\n",
    "                # print(res)\n",
    "                _sum += euclid_distance(y[idx], res)\n",
    "                num += 1\n",
    "\n",
    "\n",
    "    print(\"Average distance: \", round(_sum/num, 3))\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6153846 ,  0.648     ,  0.09090909, -0.09281667,  1.        ,\n",
       "        0.11434303,  0.5539381 ], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for dimension 0 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [65], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(res, \u001b[39mabs\u001b[39m((m(torch\u001b[39m.\u001b[39;49mfrom_numpy(data[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m][\u001b[39m0\u001b[39;49m])) \u001b[39m-\u001b[39;49m m(torch\u001b[39m.\u001b[39;49mfrom_numpy(data[i][\u001b[39m0\u001b[39;49m])))[\u001b[39m7\u001b[39;49m]))\n\u001b[0;32m      7\u001b[0m     \u001b[39m# print(m(torch.from_numpy(data[i + 1][0])) - m(torch.from_numpy(data[i][0])))\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(res)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 7 is out of bounds for dimension 0 with size 7"
     ]
    }
   ],
   "source": [
    "data = test_set[:101]\n",
    "import random\n",
    "random.shuffle(data)\n",
    "res = 0\n",
    "for i in range(100):\n",
    "    res = max(res, abs((m(torch.from_numpy(data[i + 1][0])) - m(torch.from_numpy(data[i][0])))[5]))\n",
    "    # print(m(torch.from_numpy(data[i + 1][0])) - m(torch.from_numpy(data[i][0])))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging lyrics and songs with vectors (?? I hope i won't need that in next version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = []\n",
    "\n",
    "for el, vec in zip(song_data, vecs):\n",
    "    song, artist, meta = el[0], el[1], vec[1]\n",
    "    merged.append({'song': song,\n",
    "                   'artist': artist,\n",
    "                   'meta': meta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11559"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(merged)):  # convert to list for serialization\n",
    "    merged[i]['meta'] = list(map(float, merged[i]['meta']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"meta+song.json\", 'w') as f:\n",
    "    json.dump(merged, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fba6ffd3d5c7d1161fadb8205652d886bfc0c050f44cc6ebd23b73098b1c521e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
